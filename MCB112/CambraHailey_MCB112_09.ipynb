{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write a simulator as a positive control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify parameters to be used in simulation\n",
    "\n",
    "#specify number of transcripts to initialize random probability vector \n",
    "K = 10\n",
    "\n",
    "#specify random probability vector for the transcript abundance using Dirichlet \n",
    "tau = np.random.dirichlet(np.ones(K))\n",
    "\n",
    "#specify transcript identities\n",
    "transcripts = ['Arc1', 'Arc2', 'Arc3', 'Arc4', 'Arc5', 'Arc6', 'Arc7', 'Arc8', 'Arc9', 'Arc10']\n",
    "\n",
    "#specify segment identities\n",
    "Segs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "\n",
    "#specify the number of read counts we want to simulate\n",
    "N = 1000000\n",
    "\n",
    "#specify the isoform lengths in terms of number of segments (in same order as transcript isoforms)\n",
    "#change if you want!\n",
    "L = [4, 2, 3, 4, 4, 3, 2, 2, 3, 3]\n",
    "\n",
    "#change transcript abundances to nt abundances\n",
    "v = np.divide(np.multiply(tau, L), np.sum(np.multiply(tau, L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_read_counts(N, transcripts, v, L):\n",
    "    \n",
    "    #specify seed to keep consistent random number generation\n",
    "    np.random.seed(10)\n",
    "\n",
    "    #specify empty reads matrix based on number of segments\n",
    "    reads = np.zeros(len(Segs))\n",
    "\n",
    "    #simulate N read counts from the reads\n",
    "    for i in range(N):\n",
    "        #choose a transcript\n",
    "        t = np.random.choice(len(transcripts), p=v)\n",
    "        #get length of chosen transcript\n",
    "        l = L[t] \n",
    "        #choose segment \n",
    "        s = np.random.randint(t,t+l)\n",
    "        #account for circularity\n",
    "        if s > 9:\n",
    "            s = s - 10\n",
    "        reads[s] = reads[s] + 1 \n",
    "    \n",
    "    return reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reads = simulate_read_counts(N, transcripts, v, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('read counts:', array([101026., 181175., 158928.,  95096.,  36009.,  51471., 116963.,\n",
      "       160113.,  61739.,  37480.]), 'sum of read counts:', 1000000.0, 'nt abundances:', array([0.25559672, 0.16202955, 0.04288536, 0.06879471, 0.0167199 ,\n",
      "       0.09123402, 0.12994345, 0.12014053, 0.00509722, 0.10755853]), 'probabilities:', array([0.17103957, 0.21685305, 0.03826388, 0.04603587, 0.01118858,\n",
      "       0.08140233, 0.17391046, 0.16079067, 0.00454792, 0.09596766]))\n"
     ]
    }
   ],
   "source": [
    "print(\"read counts:\", reads, \"sum of read counts:\", np.sum(reads), \"nt abundances:\", v, \"probabilities:\", tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def llh_calculation(reads, transcripts, v, L):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the loglikelihood of the observed read counts given the reads, transcript isoforms, nucleotide \n",
    "    abundances, and the length of each transcript in the number of segements. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    reads: numpy array\n",
    "        read counts of each segment corresponding to a transcript isoform.\n",
    "    \n",
    "    transcripts: list\n",
    "        transcript isoform identities\n",
    "    \n",
    "    v: numpy array\n",
    "        nucleotide abundances of each transcript isoform \n",
    "    \n",
    "    L: list\n",
    "        number of locus segments that make up each transcript isoform\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    llh: numpy float (negative)\n",
    "        loglikelihood of the observed read counts given the parameters specified above\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize loglikelihood\n",
    "    llh = 0\n",
    "    #need to marginalize out the probability of transcript and probability of segment to get P(read|v)\n",
    "    for ri in range(len(reads)):  \n",
    "        #initialize p at zero \n",
    "        pi = 0\n",
    "        for ti in range(len(transcripts)):\n",
    "            #iterate through segments of the chosen transcript \n",
    "            for si in range(ti, ti + L[ti]):\n",
    "                #circularity\n",
    "                if si > 9:\n",
    "                    si = si - 10\n",
    "                #check to see that the read is actually corresponding to the 'true' segment \n",
    "                if ri == si:\n",
    "                    pi = pi + v[ti]/np.float(L[ti])\n",
    "        #account for the number of read counts per read in llh and log pi        \n",
    "        llh = llh + np.log(pi)*reads[ri]  \n",
    "    return llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llh_me = llh_calculation(reads, transcripts, v, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2168867.9609611654"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llh_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101026., 181175., 158928.,  95096.,  36009.,  51471., 116963.,\n",
       "       160113.,  61739.,  37480.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reminder of the simulated reads\n",
    "reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remind ourselves of Lestrade's code now\n",
    "T = len(transcripts)\n",
    "\n",
    "S = len(Segs)\n",
    "\n",
    "r = reads\n",
    "\n",
    "segusage = np.zeros(S).astype(int)\n",
    "for i in range(T):\n",
    "    for j in range(i,i+L[i]): \n",
    "        segusage[j%S] += 1\n",
    "\n",
    "\n",
    "# Naive analysis:\n",
    "c  = np.zeros(T)\n",
    "for i in range(T):\n",
    "    for k in range(i,i+L[i]):\n",
    "        c[i] += (1.0 / float(segusage[k%S])) * float(r[k%S])  # For each read k, assume read k-> segment j,\n",
    "                                                              # and assign 1/usage count to each transcript\n",
    "                                                              # that contains segment j.\n",
    "Z       = np.sum(c)\n",
    "est_nu  = np.divide(c, Z)       # nucleotide abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17874167, 0.11336767, 0.09667767, 0.09009942, 0.098429  ,\n",
       "       0.086426  , 0.069269  , 0.07089775, 0.08328483, 0.112807  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lestrade's estimated nuceleotide abundance for each transcript isoform\n",
    "est_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25559672, 0.16202955, 0.04288536, 0.06879471, 0.0167199 ,\n",
       "       0.09123402, 0.12994345, 0.12014053, 0.00509722, 0.10755853])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#true nucleotide abundances for each transcript isoform from the positive control set in part 1\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use Lestrade's nucleotide abundance and see how it affects the llh\n",
    "llh_lestrade = llh_calculation(reads, transcripts, est_nu, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2208986.6054417607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llh_lestrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2168867.9609611654, -2208986.6054417607, 40118.644480595365)\n"
     ]
    }
   ],
   "source": [
    "print(llh_me, llh_lestrade, llh_me - llh_lestrade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lestrade's estimated nucleotide abundance of each transcript isoform is clearly contributing to his method's poor performance when applying the loglikelihood calculation. When Lestrade is calculating the nucleotide abundance he is basically looking at a segment and if that segment is shared among multiple transcripts, he is assuming that each of the transcripts equally contributed to the segment count, affecting the isoform abundances in the end and making his method perform poorly. In the next step, I will do a better job of estimating the isoform abundances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estimate isoform abundances by EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in Lestrade's data - thank you Sean Eddy and Lestrade :)\n",
    "\n",
    "with open(\"w09-data.out.txt\") as f:\n",
    "    #   The first line is \"The <n> transcripts of the sand mouse Arc locus\"\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'^The (\\d+) transcripts', line)\n",
    "    T     = int(match.group(1))\n",
    "\n",
    "    # The next T lines are \n",
    "    #   <Arcn>  <true_tau> <L> <structure>\n",
    "    # tau's may be present, or obscured (\"xxxxx\")\n",
    "    tau       = np.zeros(T)\n",
    "    L         = np.zeros(T).astype(int)\n",
    "    tau_known = True   # until we see otherwise\n",
    "    for i in range(T):\n",
    "        fields    = f.readline().split()\n",
    "        if fields[1] == \"xxxxx\":\n",
    "            tau_known = False\n",
    "        else:\n",
    "            tau[i] = float(fields[1])\n",
    "        L[i]      = int(fields[2])\n",
    "\n",
    "    # after a blank line,\n",
    "    # 'The <n> read sequences':\n",
    "    line  = f.readline()\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'The (\\d+) read sequences', line)\n",
    "    N     = int(match.group(1))\n",
    "\n",
    "    # the next T lines are \n",
    "    #  <read a-j> <count>\n",
    "    r = np.zeros(T).astype(int)\n",
    "    for k in range(T):\n",
    "        fields = f.readline().split()\n",
    "        r[k]   = fields[1]\n",
    "\n",
    "\n",
    "S = T    # S = R = T : there are T transcripts (Arc1..Arc10), S segments (A..J), R reads (a..j)\n",
    "R = T\n",
    "Slabel   = list(string.ascii_uppercase)[:S]               # ['A'..'J']        : the upper case labels for Arc locus segments \n",
    "Tlabel   = [ \"Arc{}\".format(d) for d in range(1,T+1) ]    # ['Arc1'..'Arc10'] : the labels for Arc transcript isoforms\n",
    "Rlabel   = list(string.ascii_lowercase)[:T]               # ['a'..'j']        : lower case labels for reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104299, 124988, 120019, 140129,  80599, 124526, 132530, 108319,\n",
       "        34583,  30008])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_calculation(T, r, n, L):\n",
    "    \n",
    "    \"\"\"\n",
    "    Expectation maximization to maximize the nucleotide abundance given the read count data parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T: integer\n",
    "        number of transcripts\n",
    "    \n",
    "    r: numpy array\n",
    "        read counts\n",
    "    \n",
    "    n: integer \n",
    "        number of times to iterate through EM calculations\n",
    "    \n",
    "    L: list\n",
    "        number of segments for each transcript\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    my_nu: numpy array\n",
    "        maximized array of nucleotide abundances for each read\n",
    "    \n",
    "    \"\"\"\n",
    "    my_nu = np.random.dirichlet(np.ones(K))\n",
    "\n",
    "    for iterate in range(n):\n",
    "        counts = np.zeros(T)\n",
    "        #expectation step\n",
    "        #need to marginalize out the probability of transcript and probability of segment to get P(read|v)\n",
    "        for ri in range(len(r)):  \n",
    "            #need probability matrix for each transcript \n",
    "            pi = np.zeros(T)\n",
    "            for ti in range(T):\n",
    "                #iterate through segments of the chosen transcript \n",
    "                for si in range(ti, ti + L[ti]):\n",
    "                    #circularity\n",
    "                    if si > 9:\n",
    "                        si = si - 10\n",
    "                    #check to see that the read is actually corresponding to the 'true' segment \n",
    "                    if ri == si:\n",
    "                    #change our 'true' nucleotide abundance to the estimated nucleotide abundance \n",
    "                        pi[ti] = pi[ti] + my_nu[ti]/np.float(L[ti])\n",
    "            #normalize probabilities\n",
    "            pi = np.divide(pi, sum(pi))\n",
    "            for tj in range(T):\n",
    "                counts[tj] = counts[tj] + r[ri]*pi[tj]\n",
    "        #maximization step\n",
    "        my_nu = np.divide(counts, sum(counts))\n",
    "\n",
    "    return my_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_nu = EM_calculation(T, r, 10000, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_new_nu = EM_calculation(T, r, 100000, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.297164 , 0.059373 , 0.0481245, 0.199186 , 0.059044 , 0.1799055,\n",
       "       0.016008 , 0.051171 , 0.0269925, 0.0630315])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llh_my_nu = llh_calculation(r, transcripts, my_nu, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "llh_my_new_nu = llh_calculation(r, transcripts, my_new_nu, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "llh_est_nu = llh_calculation(r, transcripts, est_nu, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2218066.831235943, -2218066.8312359434, -2262837.3343829094)\n"
     ]
    }
   ],
   "source": [
    "print(llh_my_nu, llh_my_new_nu, llh_est_nu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that my llh has converged between the two different iterations of the EM calculation (10000 v 100000) so I can be confident that the optimal llh has been reached. As we can see, my true nucleotide abundance gives better results than Lestrade's estimated abundance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104299, 124988, 120019, 140129,  80599, 124526, 132530, 108319,\n",
       "        34583,  30008])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to use real read counts so re-define est_nu as est_nu_r\n",
    "T = len(transcripts)\n",
    "\n",
    "S = len(Segs)\n",
    "\n",
    "r = r\n",
    "\n",
    "segusage = np.zeros(S).astype(int)\n",
    "for i in range(T):\n",
    "    for j in range(i,i+L[i]): \n",
    "        segusage[j%S] += 1\n",
    "\n",
    "\n",
    "# Naive analysis:\n",
    "c  = np.zeros(T)\n",
    "for i in range(T):\n",
    "    for k in range(i,i+L[i]):\n",
    "        c[i] += (1.0 / float(segusage[k%S])) * float(r[k%S])  # For each read k, assume read k-> segment j,\n",
    "                                                              # and assign 1/usage count to each transcript\n",
    "                                                              # that contains segment j.\n",
    "Z       = np.sum(c)\n",
    "est_nu_r  = np.divide(c, Z)       # nucleotide abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.163145  , 0.081669  , 0.11358233, 0.14821717, 0.12858725,\n",
       "       0.10172092, 0.06021225, 0.04437125, 0.06706183, 0.091433  ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lestrade's est_nu_r\n",
    "est_nu_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the nucleotide abundances back to transcript abundances\n",
    "tau_my_new_nu = np.divide(np.divide(my_new_nu, L), np.sum(np.divide(my_new_nu, L)))\n",
    "tau_est_nu_r = np.divide(np.divide(est_nu_r, L), np.sum(np.divide(est_nu_r, L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.241093  , 0.09634017, 0.05205871, 0.16160218, 0.04790316,\n",
       "       0.19461288, 0.025975  , 0.08303139, 0.02919915, 0.06818436])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_my_new_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12445724, 0.12460447, 0.1155303 , 0.11306935, 0.09809442,\n",
       "       0.10346546, 0.09186736, 0.06769834, 0.06821196, 0.0930011 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_est_nu_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that Lestrade is incorrect about his statement claiming that the transcripts are all expressed at about the same level - as they are not all expressed at the same level. Arc7 and Arc9 appear to be expressed at much lower expression levels than Lestrade reports. The most abundant two trasncripts are Arc1 and Arc6 representing over 40% of the population. Lestrade should have accounted for the fact that not all of the transcripts share segments equally, which I have done through calculating (and normalizing) the probability of the read coming from different transcripts for a given segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nu_sim = EM_calculation(T, reads, 10000, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "llh_nu_sim = llh_calculation(reads, transcripts, nu_sim, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "llh_me_sim = llh_calculation(reads, transcripts, v, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "llh_L_sim = llh_calculation(reads, transcripts, est_nu, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2168862.373198392"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llh_nu_sim # EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2168867.9609611654"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llh_me_sim # Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2208986.6054417607"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llh_L_sim # Lestrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the simulated data, it seems  that my method works better than Lestrades using simulated nucleotide abundances, as the llh is almost exactly the same as the llh with the true nucleotide abundances of the simulated data. It seems that my llh with simulated abundances may even be better than the llh with the true abundances, probably because of stochasticity (noise in true tau) in the simulation. And if Lestrade's method were actually better or just as good, we would see similar llh values here in the simulated case, but we don't. So, I am confident that my method is better than his. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
