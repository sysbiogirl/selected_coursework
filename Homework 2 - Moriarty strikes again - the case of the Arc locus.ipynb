{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json as js\n",
    "import textwrap\n",
    "import random\n",
    "import collections\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alt_map = {'-':'0'}\n",
    "complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'} \n",
    "\n",
    "def reverse_complement(seq):    \n",
    "    for k,v in alt_map.items():\n",
    "        seq = seq.replace(k,v)\n",
    "    bases = list(seq) \n",
    "    bases = reversed([complement.get(base,base) for base in bases])\n",
    "    bases = ''.join(bases)\n",
    "    for k,v in alt_map.items():\n",
    "        bases = bases.replace(v,k)\n",
    "    return bases\n",
    "\n",
    "def divide_chunks(l, n): \n",
    "#l can be list or string, n is fraglen\n",
    "# looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Run kallisto and 2. Reproduce Moriarty's Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See installation notes in Homework 2 folder. \n",
    "\n",
    "First instinct is that Moriarty failed to appreciate that directionality matters in sequencing these transcripts because each read is approximately 0.1x the length of each transcript fragment (1000 nt). \n",
    "\n",
    "I used the following arguments for each kallisto command for obtaining the data in the cells below:\n",
    "\n",
    "1. Getting the index: kallisto index split_genome -i \"Moriarty_index\" -k 31 arc.fasta\n",
    "2. Getting the abundance results: kallisto quant -i \"Moriarty_index\" -o \"Moriarty_quant\" --single -l 150 -s 20 arc.fastq.gz\n",
    "\n",
    "Note that I used the masplit_genomeimum number of k-mers possible for generating the indesplit_genome. I am interested to see if using a smaller k-mer will result in some sort of difference in the quantification step. I need to read the rest of the Pachter paper (and how de Bruijn graphs are constructed) to understand how this impacts the indesplit_genome generation and subsequently quantification of the RNA-Seq data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in results of kallisto run\n",
    "Moriarty_info_dict = {}\n",
    "with open(\"/Volumes/Macintosh HD/Users/Hailey/Dropbox/MCB112/Homework_2/Moriarty_quant/run_info.json\") as info_file:\n",
    "    info_list = info_file.readlines()\n",
    "    info = ''.join(info_list)\n",
    "    Moriarty_info_dict = js.loads(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'call': 'kallisto quant -i Moriarty_index -o Moriarty_quant --single -l 150 -s 20 arc.fastq.gz',\n",
       " 'index_version': 10,\n",
       " 'kallisto_version': '0.44.0',\n",
       " 'n_bootstraps': 0,\n",
       " 'n_processed': 100000,\n",
       " 'n_pseudoaligned': 99988,\n",
       " 'n_targets': 10,\n",
       " 'n_unique': 724,\n",
       " 'p_pseudoaligned': 100.0,\n",
       " 'p_unique': 0.7,\n",
       " 'start_time': 'Fri Sep 21 08:57:37 2018'}"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Moriarty_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make pandas dataframe of abundance of Arc loci\n",
    "abundance_Moriarty_df = pd.read_csv(\"/Volumes/Macintosh HD/Users/Hailey/Dropbox/MCB112/Homework_2/Moriarty_quant/abundance.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_id  length  eff_length  est_counts       tpm\n",
      "    Arc1    4000        3851     2382.22   17374.2\n",
      "    Arc2    2000        1851     3772.62   57244.5\n",
      "    Arc3    3000        2851    28342.40  279214.0\n",
      "    Arc4    4000        3851    10476.80   76410.3\n",
      "    Arc5    4000        3851    12679.70   92476.6\n",
      "    Arc6    3000        2851     1770.75   17444.5\n",
      "    Arc7    2000        1851     5444.64   82615.2\n",
      "    Arc8    2000        1851     5871.52   89092.5\n",
      "    Arc9    3000        2851     2649.55   26101.9\n",
      "   Arc10    3000        2851    26597.80  262027.0\n"
     ]
    }
   ],
   "source": [
    "#reproduced Moriarty's results:\n",
    "print(abundance_Moriarty_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On kallisto results - tpms look very similar to Moriarty's results. It appears I have replicated his results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simulate an Arc transcriptome and RNA-seq reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the Arc locus \n",
    "#\n",
    "S         = 10           # Number of segments in the Arc locus (A..J)\n",
    "T         = S            # Number of different transcripts (the same, one starting on each segment, 1..10)\n",
    "N         = 100000       # total number of observed reads we generate\n",
    "alpha     = 0.999        # base calling accuracy (Q30 bases, typical of current Illumina)\n",
    "len_S     = 1000         # length of each segment (nucleotides)\n",
    "len_Arc   = len_S * S    # total length of the Arc locus (nucleotides)\n",
    "len_R     = 75           # read length\n",
    "mean_frag = 150          # fragment size: mean (of a truncated Gaussian)\n",
    "sd_frag   = 20           # fragment size: stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assuming here that each Arc locus size is 2000-4000 nt long as stated in the homework esplit_genomeplanation\n",
    "Arc_size = [2000, 3000, 4000]\n",
    "\n",
    "#nucleotide array for generating sequences\n",
    "nts = ['A', 'T', 'C', 'G']\n",
    "\n",
    "#generate random segment lengths\n",
    "L_rand = pd.Series(np.random.choice(Arc_size, S))\n",
    "\n",
    "#generate 10,000 bp long random sequence of nucleotides (nts) with each having a 0.25 probability of occuring\n",
    "Arc_genome = np.random.choice(nts, 10000, p=[0.25, 0.25, 0.25, 0.25])\n",
    "#generate random, but positive vis\n",
    "v_rand = pd.Series(np.absolute(np.random.rand(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set lists for transcripts and their abundances\n",
    "Arc_transcripts = ['Arc1', 'Arc2', 'Arc3', 'Arc4', 'Arc5', 'Arc6', 'Arc7', 'Arc8', 'Arc9', 'Arc10']\n",
    "\n",
    "#create list of already used segments for simulation and concatenate to new random sequences\n",
    "Seg_cov = ['ABCD', 'BC', 'CDE', 'DEFG', 'EFGH', 'FGH', 'GH', 'HI', 'IJA', 'JAB']\n",
    "transcript_seg = {}\n",
    "transcript_seg = dict(zip(Arc_transcripts, Seg_cov))\n",
    "\n",
    "#simulations using original data\n",
    "\n",
    "v_i = [0.008, 0.039, 0.291, 0.112, 0.127, 0.008, 0.059, 0.06, 0.022, 0.273]\n",
    "\n",
    "#normalize abundances to have a total probability of 1\n",
    "c = 1/sum(v_i)\n",
    "v_i_n = [i*c for i in v_i]\n",
    "\n",
    "#sample transcript i according to its abundance\n",
    "Arc_transcript_samples_10 = np.random.choice(Arc_transcripts, size=10, p=v_i_n)\n",
    "Arc_transcript_samples_20 = np.random.choice(Arc_transcripts, size=20, p=v_i_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make dict of transcripts and store in FASTA file (same length as indicated in the original set)\n",
    "genome_sequence = list(Arc_genome)\n",
    "\n",
    "#prepare each of the transcripts with linear coordinates from original data table\n",
    "#extraction from random sequence with same coordinates as OG genome\n",
    "Arc1 = ''.join(genome_sequence[0:4000])\n",
    "\n",
    "Arc2 = ''.join(genome_sequence[1000:3000])\n",
    "\n",
    "Arc3 = ''.join(genome_sequence[2000:5000])\n",
    "\n",
    "Arc4 = ''.join(genome_sequence[3000:7000])\n",
    "\n",
    "Arc5 = ''.join(genome_sequence[4000:8000])\n",
    "\n",
    "Arc6 = ''.join(genome_sequence[5000:8000])\n",
    "\n",
    "Arc7 = ''.join(genome_sequence[6000:8000])\n",
    "\n",
    "Arc8 = ''.join(genome_sequence[7000:9000])\n",
    "\n",
    "Arc9 = ''.join(genome_sequence[8000:10000] + genome_sequence[0:1000])\n",
    "\n",
    "Arc10 = ''.join(genome_sequence[9000:10000] + genome_sequence[0:2000])\n",
    "\n",
    "#initialize ordered dict for the transcript dictionary so that Arc10 is actually last in file when written\n",
    "transcript_dict = collections.OrderedDict()\n",
    "transcript_dict['Arc1'] = Arc1\n",
    "transcript_dict['Arc2'] = Arc2\n",
    "transcript_dict['Arc3'] = Arc3\n",
    "transcript_dict['Arc4'] = Arc4\n",
    "transcript_dict['Arc5'] = Arc5\n",
    "transcript_dict['Arc6'] = Arc6\n",
    "transcript_dict['Arc7'] = Arc7\n",
    "transcript_dict['Arc8'] = Arc8\n",
    "transcript_dict['Arc9'] = Arc9\n",
    "transcript_dict['Arc10'] = Arc10\n",
    "\n",
    "#write out fasta file    \n",
    "fasta_rand = open('Arc_og_length_rand_seq.fasta', 'w')\n",
    "\n",
    "for i in range(len(transcript_dict)):\n",
    "    \n",
    "    #make sure to get proper formatting for FASTA\n",
    "    fasta_rand.write('>' + list(transcript_dict.keys())[i] + '\\n' + '\\n'.join(textwrap.wrap(list(transcript_dict.values())[i], 60)) + '\\n')\n",
    "\n",
    "#do not forget to close it\n",
    "fasta_rand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out fastq file    \n",
    "fastq = open('Arc_og_len_2.fastq', 'w')\n",
    "for x in range(10000):\n",
    "    #choose randomly sized fragment from sample transcript\n",
    "    Arc_transcript_samples_10 = np.random.choice(Arc_transcripts, size=10, p=v_i_n)\n",
    "\n",
    "    rand_transcripts = []\n",
    "    for transcript in Arc_transcript_samples_10:\n",
    "        for Arc in list(transcript_dict.keys()):\n",
    "            if str(transcript) == Arc:\n",
    "                #print(transcript)\n",
    "                while True:\n",
    "                    fraglen = int(np.random.normal(mean_frag, sd_frag))\n",
    "                    if fraglen >= len_R: break\n",
    "                if fraglen > len(transcript_dict['{}'.format(transcript)]): fraglen = len(transcript_dict['{}'.format(transcript)])\n",
    "                if fraglen < 75 : fraglen = 75\n",
    "                x = list(divide_chunks(str(list(transcript_dict.values())), fraglen))\n",
    "                y = len(x)\n",
    "                z = np.random.choice(y-1, 1)\n",
    "                rand_transcripts.append(x[int(z)])\n",
    "\n",
    "    fragments = rand_transcripts\n",
    "    rand_reads = []\n",
    "    for frag in fragments:\n",
    "        #print(len(frag))\n",
    "        j = np.random.randint(len(frag))\n",
    "        read_f = frag[j: j+75]\n",
    "        reads_f.append(read_f)\n",
    "        read_r = frag[len(frag)-75: len(frag)]\n",
    "        reads_r.append(reverse_complement(read_r))\n",
    "        read_r = reverse_complement(read_r)\n",
    "        x = np.random.randint(2)\n",
    "        if x == 0:\n",
    "            read = read_r\n",
    "        if x == 1:\n",
    "            read = read_f\n",
    "        rand_reads.append(read)\n",
    "\n",
    "    #introduce errors into reads\n",
    "    nts = ['A', 'T', 'C', 'G']\n",
    "    p_A = np.array([alpha, 1-alpha, 1-alpha, 1-alpha])\n",
    "    p_A /= p_A.sum()\n",
    "\n",
    "    p_T = np.array([1-alpha, alpha, 1-alpha, 1-alpha])\n",
    "    p_T /= p_T.sum()\n",
    "\n",
    "    p_C = np.array([1-alpha, 1-alpha, alpha, 1-alpha])\n",
    "    p_C /= p_C.sum()\n",
    "\n",
    "    p_G = np.array([1-alpha, 1-alpha, 1-alpha, alpha])\n",
    "    p_G /= p_G.sum()\n",
    "\n",
    "    rand_read_err = []\n",
    "    for read in rand_reads:\n",
    "        for i in range(len(read)):\n",
    "            if read[i] == nts[0]:\n",
    "                read[i].replace(read[i], np.random.choice(nts, p=p_A))\n",
    "            elif read[i] == nts[1]:\n",
    "                read[i].replace(read[i], np.random.choice(nts, p=p_T))\n",
    "            elif read[i] == nts[2]:\n",
    "                read[i].replace(read[i], np.random.choice(nts, p=p_C))\n",
    "            elif read[i] == nts[3]:\n",
    "                read[i].replace(read[i], np.random.choice(nts, p=p_G))\n",
    "        rand_read_err.append(read)\n",
    "    #write sequences with errors to FASTQ format file (in same format as one provided)\n",
    "\n",
    "    for i in range(len(rand_read_err)):\n",
    "        #make sure to get proper formatting for FASTA\n",
    "        fastq.write('@read{}'.format(i) + '\\n' + rand_read_err[i] + '\\n' + '+' + '\\n' + '?'*int(75) + '\\n')\n",
    "    #do not forget to close it\n",
    "fastq.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make list of random (plausible) loci of the Arc genome\n",
    "\n",
    "#make string long enough so that I don't end up getting empty strings when choosing randomly (but in order of locus)\n",
    "arc_str = 'ABCDEFGHIJABCDEFGHIJABCDEFGHIJABCDEFGHIJABCDEFGHIJABCDEFGHIJABCDEFGHIJABCDEFGHIJ'\n",
    "Arc_rand = []\n",
    "transcript = ''\n",
    "\n",
    "#loop through string in order to get a list of random locuses\n",
    "j=0\n",
    "for arc_seg in range(10):\n",
    "    j = j + np.random.randint(2,5)\n",
    "    transcript = ''.join(np.random.choice([arc_str[j :j + np.random.randint(2,5)]]))\n",
    "    Arc_rand.append(transcript)\n",
    "        \n",
    "Arc_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The circular architecture of the genome and how the reads are sampled from the circular architecture may be causing an issue with how kallisto records reads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "#choose 75 nt sample read from fragment in random orientation\n",
    "j = 0        \n",
    "for i in range(0, len(fragments), 75):\n",
    "    for frag in fragments:\n",
    "        j = np.random.randint(len(frag) - 75)\n",
    "        read_f = frag[j: j+75]\n",
    "        reads_f.append(read_f)\n",
    "        read_r = frag[len(frag)-75: len(frag)]\n",
    "        reads_r.append(reverse_complement(read_r))\n",
    "        read_r = reverse_complement(read_r)\n",
    "        x = np.random.randint(2)\n",
    "        if x == 0:\n",
    "            read = read_r\n",
    "        if x == 1:\n",
    "            read = read_f\n",
    "\n",
    "#print(reads)\n",
    "#make reads for FASTQ file\n",
    "#be sure to add errors\n",
    "#rand_reads = []\n",
    "#for i in range(100000):\n",
    "    #random_read = np.random.choice(reads)\n",
    "    #rand_reads.append(random_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#introduce errors into reads\n",
    "nts = ['A', 'T', 'C', 'G']\n",
    "p_A = np.array([alpha, 1-alpha, 1-alpha, 1-alpha])\n",
    "p_A /= p_A.sum()\n",
    "\n",
    "p_T = np.array([1-alpha, alpha, 1-alpha, 1-alpha])\n",
    "p_T /= p_T.sum()\n",
    "\n",
    "p_C = np.array([1-alpha, 1-alpha, alpha, 1-alpha])\n",
    "p_C /= p_C.sum()\n",
    "\n",
    "p_G = np.array([1-alpha, 1-alpha, 1-alpha, alpha])\n",
    "p_G /= p_G.sum()\n",
    "\n",
    "rand_read_err = []\n",
    "for read in rand_reads:\n",
    "    for i in range(len(read)):\n",
    "        if read[i] == nts[0]:\n",
    "            read[i].replace(read[i], np.random.choice(nts, p=p_A))\n",
    "        elif read[i] == nts[1]:\n",
    "            read[i].replace(read[i], np.random.choice(nts, p=p_T))\n",
    "        elif read[i] == nts[2]:\n",
    "            read[i].replace(read[i], np.random.choice(nts, p=p_C))\n",
    "        elif read[i] == nts[3]:\n",
    "            read[i].replace(read[i], np.random.choice(nts, p=p_G))\n",
    "    rand_read_err.append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write sequences with errors to FASTQ format file (in same format as one provided)\n",
    "\n",
    "#write out fastq file    \n",
    "fastq = open('Arc_og_length_100.fastq', 'w')\n",
    "for i in range(len(rand_read_err)):\n",
    "    #make sure to get proper formatting for FASTA\n",
    "    fastq.write('@read{}'.format(i) + '\\n' + rand_read_err[i] + '\\n' + '+' + '\\n' + '?'*int(75) + '\\n')\n",
    "#do not forget to close it\n",
    "fastq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of the genome with segment names as keys, splitting the genome array by the number of segments\n",
    "split_genome = np.array_split(Arc_genome, S)\n",
    "\n",
    "Arc_dict = {}\n",
    "Arc_dict['A'] = split_genome[0]\n",
    "Arc_dict['B'] = split_genome[1]\n",
    "Arc_dict['C'] = split_genome[2]\n",
    "Arc_dict['D'] = split_genome[3]\n",
    "Arc_dict['E'] = split_genome[4]\n",
    "Arc_dict['F'] = split_genome[5]\n",
    "Arc_dict['G'] = split_genome[6]\n",
    "Arc_dict['H'] = split_genome[7]\n",
    "Arc_dict['I'] = split_genome[8]\n",
    "Arc_dict['J'] = split_genome[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['G', 'F', 'B', 'E', 'H', 'C', 'I', 'J', 'A', 'D']) dict_values(['IJA', 'EFGH', 'BC', 'DEFG', 'HI', 'CDE', 'ABCD', 'GH', 'FGH', 'JAB'])\n"
     ]
    }
   ],
   "source": [
    "dict_of_dicts = {}\n",
    "\n",
    "transcript_sequences = []\n",
    "\n",
    "#use the keys in Arc_dict to assign sequences to each of the transcripts with their respective segments to a new \n",
    "#list of sequences representing each transcript\n",
    "print(Arc_dict.keys(), transcript_seg.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JJJ', 'IJ', 'ADCC', 'FIG', 'ACBH', 'AGJC', 'FGCF', 'CG', 'FCC', 'CBBE']"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create random list of Arc segment names with requirements\n",
    "Arc_seg = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "Arc_seg_len = 1000\n",
    "\n",
    "#make list of possible Arc locuses based on genome structure given\n",
    "Arc_rand = []\n",
    "transcript = ''\n",
    "for seg in Arc_seg:\n",
    "    #get random combinations of letters from the list above and concatenate randomly in groups of 2 - 4 locuses\n",
    "    transcript = ''.join(np.random.choice(Arc_seg, replace=False) for _ in range(np.random.randint(2, 5)))\n",
    "    #esplit_genomeclude empty transcripts:\n",
    "    if len(transcript) >= 2 and len(transcript) <= 4:\n",
    "             Arc_rand.append(transcript)\n",
    "Arc_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 - make dictionary of segment:bp pairs that are random\n",
    "#step 2 - make sequences of the random locuses generated from the list above, with a new dataframe\n",
    "\n",
    "Arc_seg = ['A','B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arc_transcript_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
